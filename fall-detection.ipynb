{"cells":[{"cell_type":"markdown","id":"1ab6420d","metadata":{"id":"1ab6420d"},"source":["## Fall Detection using Visual Tracking\n","### Computer Vision and Image Analysis\n","-------\n","##### Katarina Keishanti Joanne Kartakusuma - 21/472847/PA/20347\n","#### Assignment 3"]},{"cell_type":"markdown","id":"720fadca","metadata":{"id":"720fadca"},"source":["### Import Dependencies"]},{"cell_type":"code","execution_count":null,"id":"fa1fcba5","metadata":{"id":"fa1fcba5"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import tensorflow as tf"]},{"cell_type":"markdown","id":"64317d15","metadata":{"id":"64317d15"},"source":["### Drawing keypoints"]},{"cell_type":"code","execution_count":null,"id":"d13dedd4","metadata":{"id":"d13dedd4"},"outputs":[],"source":["def draw_kp(frame, keypoints, confidence_thresh):\n","    y, x, c = frame.shape\n","    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n","\n","    for kp in shaped:\n","        ky, kx, kp_conf = kp\n","        if kp_conf > confidence_thresh:\n","            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 255, 0), -1)"]},{"cell_type":"markdown","id":"82429eb5","metadata":{"id":"82429eb5"},"source":["### Connecting the keypoints"]},{"cell_type":"code","execution_count":null,"id":"3f1e81a5","metadata":{"id":"3f1e81a5"},"outputs":[],"source":["EDGES = {\n","    (0, 1): \"m\",\n","    (0, 2): \"c\",\n","    (1, 3): \"m\",\n","    (2, 4): \"c\",\n","    (0, 5): \"m\",\n","    (0, 6): \"c\",\n","    (5, 7): \"m\",\n","    (7, 9): \"m\",\n","    (6, 8): \"c\",\n","    (8, 10): \"c\",\n","    (5, 6): \"y\",\n","    (5, 11): \"m\",\n","    (6, 12): \"c\",\n","    (11, 12): \"y\",\n","    (11, 13): \"m\",\n","    (13, 15): \"m\",\n","    (12, 14): \"c\",\n","    (14, 16): \"c\",\n","}"]},{"cell_type":"code","execution_count":null,"id":"462df518","metadata":{"id":"462df518"},"outputs":[],"source":["def kp_connections(frame, keypoints, edges, confidence_thresh):\n","    y, x, c = frame.shape\n","    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n","\n","    for edge, color in edges.items():\n","        p1, p2 = edge\n","        y1, x1, c1 = shaped[p1]\n","        y2, x2, c2 = shaped[p2]\n","\n","        if (c1 > confidence_thresh) & (c2 > confidence_thresh):\n","            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)"]},{"cell_type":"markdown","id":"36f45f76","metadata":{"id":"36f45f76"},"source":["### Fall detection"]},{"cell_type":"code","execution_count":null,"id":"fd512f8e","metadata":{"id":"fd512f8e"},"outputs":[],"source":["def detect_falls(video_path):\n","    interpreter = tf.lite.Interpreter(model_path=\"/Users/katarinajoanne/University/UGM/2 - Second Year/Fourth Semester/Computer Vision and Image Analysis/Assignment-3/lite-model_movenet_singlepose_lightning_3.tflite\")\n","    interpreter.allocate_tensors()\n","\n","    cap = cv2.VideoCapture(video_path)\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","\n","        if not ret:\n","            break\n","\n","        # Reshape image to 192 x 192 x 3\n","        img = frame.copy()\n","        img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192, 192)\n","        input_image = tf.cast(img, dtype=tf.float32)\n","\n","        # Setup input and output\n","        input_details = interpreter.get_input_details()\n","        output_details = interpreter.get_output_details()\n","\n","        # Make predictions\n","        interpreter.set_tensor(input_details[0][\"index\"], np.array(input_image))\n","        interpreter.invoke()\n","        keypoints_with_scores = interpreter.get_tensor(output_details[0][\"index\"])\n","\n","        # Rendering\n","        kp_connections(frame, keypoints_with_scores, EDGES, 0.4)\n","        draw_kp(frame, keypoints_with_scores, 0.4)\n","\n","        landmarks_list = keypoints_with_scores[0]\n","\n","        # Check if there are landmarks\n","        if landmarks_list.shape[0] > 0:\n","            min_y = np.min(landmarks_list[:, 1])\n","            max_y = np.max(landmarks_list[:, 1])\n","\n","            if np.isnan(min_y) or np.isnan(max_y):\n","                continue\n","\n","            # Adjust the fall detection threshold here\n","            fall_threshold = 0.45\n","            \n","            print(max_y - min_y)\n","            if max_y - min_y > fall_threshold:\n","                # If fall is detected, add text to the frame\n","                cv2.putText(frame, \"Fall detected\", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2)\n","\n","        cv2.imshow(\"MoveNet Lightning\", frame)\n","\n","        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n","            break\n","\n","    cap.release()\n","    cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"dce4e2e3","metadata":{"id":"dce4e2e3"},"source":["### Testing"]},{"cell_type":"code","execution_count":null,"id":"3578b192","metadata":{"id":"3578b192","outputId":"2d1e48fa-ce0b-4298-b96b-3304c649a2de"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: Applying 1 TensorFlow Lite delegate(s) lazily.\n"]},{"name":"stdout","output_type":"stream","text":["0.3476678\n","0.32713383\n","0.35195696\n","0.39195523\n","0.39147675\n","0.31419393\n","0.2906052\n","0.280688\n","0.23341006\n","0.41338545\n","0.31842554\n","0.28146338\n","0.3685593\n","0.28407234\n","0.25404078\n","0.2394651\n","0.1978324\n","0.3092438\n","0.28858647\n","0.2976872\n","0.30154663\n","0.24915779\n","0.2479536\n","0.31968272\n","0.4895052\n","0.4614438\n","0.33695158\n","0.27919972\n","0.08504486\n","0.65303916\n","0.837806\n","0.2840091\n","0.34011954\n","0.35814255\n","0.3426305\n","0.29172313\n","0.51587576\n","0.5143788\n","0.42385086\n","0.6503502\n","0.56938666\n","0.17714322\n","0.74929833\n","0.758648\n","0.016179372\n","0.69269115\n","0.58629256\n","0.04363575\n","0.87239563\n","0.5888247\n"]}],"source":["video_path = \"/Users/katarinajoanne/University/UGM/2 - Second Year/Fourth Semester/Computer Vision and Image Analysis/Assignment-3/sample.gif\"\n","detect_falls(video_path)"]},{"cell_type":"code","execution_count":null,"id":"20bceeea","metadata":{"id":"20bceeea","outputId":"0a7957a0-8e6e-497d-a091-25a383e34d54"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO: Applying 1 TensorFlow Lite delegate(s) lazily.\n"]},{"name":"stdout","output_type":"stream","text":["0.15849406\n","0.15275475\n","0.28265154\n","0.21493101\n","0.1924288\n","0.12905645\n","0.12905645\n","0.20393464\n","0.21435338\n","0.20720987\n","0.22378325\n","0.22412595\n","0.22412595\n","0.21945733\n","0.25663456\n","0.23805955\n","0.064587444\n","0.13187283\n","0.17948335\n","0.42659009\n","0.6676614\n","0.26745445\n","0.28531328\n","0.27921888\n","0.27921888\n","0.507524\n","0.25592315\n","0.2443895\n","0.4274596\n","0.4274596\n"]}],"source":["video_path = \"/Users/katarinajoanne/University/UGM/2 - Second Year/Fourth Semester/Computer Vision and Image Analysis/Assignment-3/sample_fall.gif\"\n","detect_falls(video_path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}